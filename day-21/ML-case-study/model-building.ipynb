{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d734136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dgomonov/new-york-city-airbnb-open-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa750964",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab06bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbaaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = r\"airbnb-newyork\\AB_NYC_2019.csv\"\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "print(\"Loaded:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfff988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99249331",
   "metadata": {},
   "source": [
    "## 3. Basic Cleaning\n",
    "- Remove listings with missing price or zero price\n",
    "- Remove extreme prices (> $1000) for stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066305d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['price'] > 0]\n",
    "df = df[df['price'] <= 1000]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"After filtering:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193ac06",
   "metadata": {},
   "source": [
    "## 4. Text Feature Engineering\n",
    "We will use the `name` column as text input:\n",
    "- Length of name\n",
    "- Word count\n",
    "- Presence of certain keywords\n",
    "- Sentiment score\n",
    "- TF-IDF vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_text(x):\n",
    "    return '' if pd.isna(x) else str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_text'] = df['name'].apply(safe_text)\n",
    "df['name_len'] = df['name_text'].apply(len)\n",
    "df['name_words'] = df['name_text'].apply(lambda s: len(s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['cozy', 'luxury', 'spacious', 'modern', 'private', 'view']\n",
    "for kw in keywords:\n",
    "    df[f'kw_{kw}'] = df['name_text'].str.lower().str.contains(kw).fillna(False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49efc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_sentiment'] = df['name_text'].apply(lambda s: TextBlob(s).sentiment.polarity if s else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91361065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['name_text','name_len','name_words','name_sentiment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d7321",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "We'll keep:\n",
    "- Numeric: latitude, longitude, minimum_nights, number_of_reviews, reviews_per_month, availability_365\n",
    "- Categorical: neighbourhood_group, room_type\n",
    "- Text: name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['latitude','longitude','minimum_nights','number_of_reviews','reviews_per_month','availability_365']\n",
    "cat_cols = ['neighbourhood_group','room_type']\n",
    "text_col = 'name_text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e3e56",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split\n",
    "We use log(price) as the target to reduce skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776726f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = df[num_cols + cat_cols + [text_col]]\n",
    "y = np.log1p(df['price'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad497e",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Pipelines\n",
    "Separate numeric, categorical, and text transformations.\n",
    "\n",
    "The main goals of preprocessing are:\n",
    "\n",
    "Handle missing values\n",
    "\n",
    "Transform features into model-friendly formats\n",
    "\n",
    "Ensure consistent scaling and encoding\n",
    "\n",
    "Reduce dimensionality when dealing with high-dimensional text## 0. Understanding the Preprocessing Stages\n",
    "\n",
    "Before we train any model, we must prepare our dataset so that it is:\n",
    "- Free of missing values\n",
    "- All features are in numeric form\n",
    "- Scaled appropriately (for models that need it)\n",
    "- Reduced in dimensionality when working with high-dimensional data (like text)\n",
    "\n",
    "We build **three pipelines**:\n",
    "1. **Numeric Pipeline** — Median imputation and scaling\n",
    "2. **Categorical Pipeline** — Constant imputation and One-Hot Encoding\n",
    "3. **Text Pipeline** — TF-IDF vectorization + Truncated SVD\n",
    "\n",
    "Finally, we combine them with a `ColumnTransformer` so they run in parallel.\n",
    "\n",
    "The goal:\n",
    "- **Numeric** → handle missing, scale for algorithms like Ridge  \n",
    "- **Categorical** → convert text labels into binary dummy columns  \n",
    "- **Text** → convert long text into structured numeric form while keeping the most useful components  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba56dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a47c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "text_pipeline_ridge = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=2000, stop_words='english')),\n",
    "    ('svd', TruncatedSVD(n_components=50, random_state=42))\n",
    "])\n",
    "\n",
    "text_pipeline_rf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),\n",
    "    ('svd', TruncatedSVD(n_components=20, random_state=42))\n",
    "])\n",
    "\n",
    "preprocessor_ridge = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_cols),\n",
    "    ('cat', categorical_pipeline, cat_cols),\n",
    "    ('text', text_pipeline_ridge, text_col)\n",
    "])\n",
    "\n",
    "preprocessor_rf = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_cols),\n",
    "    ('cat', categorical_pipeline, cat_cols),\n",
    "    ('text', text_pipeline_rf, text_col)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08476b8",
   "metadata": {},
   "source": [
    "## 8. Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371adc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pipe_ridge = Pipeline([\n",
    "    ('pre', preprocessor_ridge),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "pipe_ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = np.expm1(pipe_ridge.predict(X_test))\n",
    "\n",
    "print(\"Ridge Regression Performance:\")\n",
    "print(\"RMSE:\", mean_squared_error(np.expm1(y_test), y_pred_ridge, squared=False))\n",
    "print(\"MAE:\", mean_absolute_error(np.expm1(y_test), y_pred_ridge))\n",
    "print(\"R²:\", r2_score(np.expm1(y_test), y_pred_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b9a8c",
   "metadata": {},
   "source": [
    "## 9. Random Forest Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd40c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    ('pre', preprocessor_rf),\n",
    "    ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "y_pred_rf = np.expm1(pipe_rf.predict(X_test))\n",
    "\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"RMSE:\", mean_squared_error(np.expm1(y_test), y_pred_rf, squared=False))\n",
    "print(\"MAE:\", mean_absolute_error(np.expm1(y_test), y_pred_rf))\n",
    "print(\"R²:\", r2_score(np.expm1(y_test), y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3329556",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Ridge Regression', 'Random Forest'],\n",
    "    'RMSE': [\n",
    "        mean_squared_error(np.expm1(y_test), y_pred_ridge, squared=False),\n",
    "        mean_squared_error(np.expm1(y_test), y_pred_rf, squared=False)\n",
    "    ],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(np.expm1(y_test), y_pred_ridge),\n",
    "        mean_absolute_error(np.expm1(y_test), y_pred_rf)\n",
    "    ],\n",
    "    'R²': [\n",
    "        r2_score(np.expm1(y_test), y_pred_ridge),\n",
    "        r2_score(np.expm1(y_test), y_pred_rf)\n",
    "    ]\n",
    "})\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e98af7",
   "metadata": {},
   "source": [
    "## 12. Hyperparameter Tuning\n",
    "We will tune:\n",
    "- Ridge Regression: `alpha`\n",
    "- Random Forest: `n_estimators`, `max_depth`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ridge Regression Grid Search\n",
    "ridge_param_grid = {'model__alpha': [0.1, 1.0, 10.0, 50.0, 100.0]}\n",
    "ridge_gs = GridSearchCV(\n",
    "    Pipeline([('pre', preprocessor_ridge), ('model', Ridge())]),\n",
    "    ridge_param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "ridge_gs.fit(X_train, y_train)\n",
    "print(\"Best Ridge Params:\", ridge_gs.best_params_)\n",
    "print(\"Best Ridge RMSE:\", -ridge_gs.best_score_)\n",
    "\n",
    "# Random Forest Grid Search (small grid for speed)\n",
    "rf_param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 10, 20]\n",
    "}\n",
    "rf_gs = GridSearchCV(\n",
    "    Pipeline([('pre', preprocessor_rf), ('model', RandomForestRegressor(random_state=42, n_jobs=-1))]),\n",
    "    rf_param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print(\"Best RF Params:\", rf_gs.best_params_)\n",
    "print(\"Best RF RMSE:\", -rf_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026fac0",
   "metadata": {},
   "source": [
    "## 13. Model Explainability (Random Forest)\n",
    "We'll use:\n",
    "- **Permutation Importance** for global feature importance\n",
    "- **SHAP values** for local interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Train final Random Forest with best params\n",
    "best_rf = rf_gs.best_estimator_\n",
    "result = permutation_importance(best_rf, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names after preprocessing\n",
    "cat_feature_names = rf_gs.best_estimator_['pre'].named_transformers_['cat']['ohe'].get_feature_names_out(cat_cols)\n",
    "feature_names = num_cols + list(cat_feature_names) + [f\"svd_{i}\" for i in range(20)]\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()[-15:]\n",
    "plt.barh(np.array(feature_names)[sorted_idx], result.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.title(\"Top 15 Features - Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9d54e",
   "metadata": {},
   "source": [
    "### 14. SHAP (Local Interpretability)\n",
    "SHAP explains individual predictions by showing how each feature pushes the prediction higher or lower.\n",
    "\n",
    "\n",
    "\n",
    "#### Understanding SHAP and the Necessity of Model Explainability\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** assigns each feature a value showing how much it contributed to a prediction.\n",
    "\n",
    "Why this matters:\n",
    "- **Transparency**: We see *why* the model made a certain prediction.\n",
    "- **Trust**: Stakeholders are more confident in the model.\n",
    "- **Debugging**: We can catch when the model is using unintended signals.\n",
    "- **Compliance**: In regulated industries, explainability is not optional.\n",
    "\n",
    "SHAP is based on Shapley values from cooperative game theory, where each feature is a \"player\" in predicting the outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea089300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take a lot of time!!!\n",
    "\n",
    "import shap\n",
    "\n",
    "# Use TreeExplainer for Random Forest\n",
    "explainer = shap.TreeExplainer(best_rf['model'])\n",
    "X_test_transformed = best_rf['pre'].transform(X_test)\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "# Summary plot (requires feature names from preprocessing)\n",
    "shap.summary_plot(shap_values, features=X_test_transformed, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de6fee",
   "metadata": {},
   "source": [
    "SHAP on the full test set can be slow because:\n",
    "\n",
    "Random Forests with hundreds of trees generate many calculations per sample.\n",
    "\n",
    "SHAP’s exact method for trees can be heavy when the dataset is large.\n",
    "\n",
    "You can speed this up by:\n",
    "\n",
    "Sampling fewer rows from the test set.\n",
    "\n",
    "Using permutation importance or shap.Explainer with approximate=True for a rougher (but much faster) estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Sample only 100 rows for speed\n",
    "sample_idx = np.random.choice(len(X_test), 100, replace=False)\n",
    "X_test_sample = X_test.iloc[sample_idx]\n",
    "\n",
    "# Use TreeExplainer with the sample\n",
    "explainer = shap.TreeExplainer(best_rf['model'])\n",
    "X_test_sample_transformed = best_rf['pre'].transform(X_test_sample)\n",
    "\n",
    "# Compute SHAP values for only the sample\n",
    "shap_values_sample = explainer.shap_values(X_test_sample_transformed)\n",
    "\n",
    "# Summary plot for the sample\n",
    "shap.summary_plot(shap_values_sample, \n",
    "                  features=X_test_sample_transformed, \n",
    "                  feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074d1e0",
   "metadata": {},
   "source": [
    "## 15. Final Notes\n",
    "- Linear models + text features can work surprisingly well, especially with TF-IDF + dimensionality reduction.\n",
    "- Random Forests often outperform linear models on tabular + text-derived features but are harder to interpret.\n",
    "- Always balance performance with explainability — stakeholders may need to understand *why* a model predicts certain prices.\n",
    "- Hyperparameter tuning can make a big difference, but be mindful of overfitting and computation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f5485",
   "metadata": {},
   "source": [
    "## 16. Making a Sample Prediction\n",
    "\n",
    "Let's create a fake Airbnb listing and use our trained Random Forest to predict its price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca282ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new listing\n",
    "sample_data = pd.DataFrame([{\n",
    "    'name': 'Cozy apartment near Central Park',\n",
    "    'host_id': 999999,\n",
    "    'host_name': 'John Doe',\n",
    "    'neighbourhood_group': 'Manhattan',\n",
    "    'neighbourhood': 'Upper West Side',\n",
    "    'latitude': 40.785091,\n",
    "    'longitude': -73.968285,\n",
    "    'room_type': 'Entire home/apt',\n",
    "    'minimum_nights': 3,\n",
    "    'number_of_reviews': 15,\n",
    "    'last_review': '2023-05-10',\n",
    "    'reviews_per_month': 1.2,\n",
    "    'calculated_host_listings_count': 1,\n",
    "    'availability_365': 180\n",
    "}])\n",
    "\n",
    "# Reindex to match training columns\n",
    "sample_data = sample_data.reindex(columns=X.columns, fill_value='')\n",
    "\n",
    "# Predict\n",
    "predicted_price = best_rf.predict(sample_data)[0]\n",
    "print(f\"Predicted Price: ${predicted_price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3b07f",
   "metadata": {},
   "source": [
    "If you’ve built your model inside a pipeline — like:\n",
    "\n",
    "best_rf = Pipeline([\n",
    "    ('pre', preprocessor_rf),\n",
    "    ('model', RandomForestRegressor(...))\n",
    "])\n",
    "\n",
    "\n",
    "— then you do NOT need to manually preprocess inputs during prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
